岗位职责

1、分布式训练框架开发：主导或深度参与书生系列大模型训练框架的设计、实现与迭代，覆盖多种任务（预训练、SFT、RL等）和多种模态（语言、视频、音频等）。
2、性能极致优化：深入分析训练瓶颈，对计算（GPU/NPU）、通信（NCCL/RDMA）、存储（IO）和内存进行系统性优化，包括不限于算子融合、显存优化等技术。
3、稳定性与可靠性保障：构建高可用的训练系统，确保超大规模集群上长时间训练的稳定性。
协作与支持：与模型研究团队紧密合作，理解其需求并将其转化为框架特性，为研究员提供高性能训练解决方案和技术支持。
4、技术前瞻性研究：跟踪业界最新技术动态，探索并引入新的并行策略、编译技术和硬件协同设计，保持框架的技术领先性。

岗位要求

1、计算机科学、人工智能或相关领域的本科及以上学历。
2、拥有PyTorch开发或深度定制经验，熟悉其自动微分、计算图、张量计算等核心机制。
3、有使用Deepspeed、FSDP、Megatron等框架进行大模型分布式训练的实际项目经验。

加分项：
1、熟悉GPU/NPU硬件架构，有CUDA/OpenMPI内核开发或性能剖析经验（使用Nsight, Triton等工具）。
2、具备分布式系统知识，熟悉NCCL、MPI、gRPC等通信库，对RDMA有了解者优先。
3、参与过知名深度学习框架（PyTorch, TensorFlow）或大规模分布式训练系统的开源贡献。
4、强烈的责任心、出色的分析和解决复杂问题的能力，对高性能计算充满热情。

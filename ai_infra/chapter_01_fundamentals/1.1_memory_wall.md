# 1.1 显存墙 (Memory Wall) & Roofline Model 🧱

## 🎯 核心目标
理解为什么在大模型时代，**搬运数据比计算数据更贵**。

## 💡 概念解析

### 1. 显存墙 (The Memory Wall)
在深度学习的早期（比如 2015 年 ResNet 时代），我们的瓶颈通常是 **算力 (Compute)**。GPU 算得不够快，显存带宽通常够用。
但在 LLM 时代，情况反转了。

*   **计算能力 (FLOPs)** 增长极快 (摩尔定律)。
*   **显存带宽 (Memory Bandwidth)** 增长相对缓慢。

中间的差距，就是 **"显存墙"**。

#### 📸 CV 工程师类比
*   **ResNet-50:** 参数量 ~25M (100MB 显存)。
    *   **场景:** 一张 224x224 图片进去，Conv2d 疯狂计算，参数复用率极高。
    *   **瓶颈:** **Compute Bound (计算受限)**。GPU 核心冒烟了，显存带宽还在睡觉 💤。
*   **LLM (Llama-3-70B):** 参数量 70B (140GB+ 显存)。
    *   **场景:** 生成一个 token，需要把 140GB 的权重 **全部** 从 HBM (High Bandwidth Memory) 搬运到 GPU 核心计算一次。
    *   **瓶颈:** **Memory Bound (访存受限)**。GPU 核心在打牌我们要等数据搬过来，显存带宽已经跑满 🔥。

### 2. Roofline Model (房顶模型)
这是一个用来判断模型是 "算力瓶颈" 还是 "带宽瓶颈" 的经典图表。

*   **算术强度 (Arithmetic Intensity):**  `计算量 (FLOPs) / 访存量 (Bytes)`。
    *   也就是：**每搬运 1 Byte 数据，能进行多少次计算？**
*   **LLM 推理的特点:** 算术强度极低 (尤其是 Decoding 阶段，Batch=1 时，接近 1:1)。
*   **ResNet 训练的特点:** 算术强度高 (卷积核虽小，但在大 Feature Map 上滑动计算了很多次)。

## ⚔️ 课后实战 (Action Items)
1.  **算一算:** 假设你有一张 A100 (带宽 2TB/s)。
    *   Llama-70B (FP16, 140GB) 生成 1 个 token，最快需要多少毫秒？
    *   *答案*: `140GB / 2TB/s = 70ms`。这就是**理论极限延迟**，再怎么优化代码也无法突破物理定律。
2.  **思考:** 为什么增大 Batch Size 可以缓解显存墙问题？(提示：权重的复用率提高了)。

# 1.2 计算密集 (Compute Bound) vs 访存密集 (Memory Bound) 🐢🐇

## 🎯 核心目标
学会像医生诊断病人一样，一眼看出模型慢在哪里。是卡在**算力**还是卡在**带宽**？

## 💡 概念解析

### 1. Compute Bound (计算密集型) 🧠
*   **症状:** GPU 利用率 (Volatile GPU-Util) 持续 **100%**。风扇狂转。
*   **原因:** 算术强度高，数据搬运时间 < 计算时间。
*   **例子:** `MatMul` (矩阵乘法, GEMM), `Conv2d` (大卷积核)。
*   **优化方向:** 使用 Tensor Core, 量化 (FP8/INT8) 减少计算量，算子融合。

#### 📸 CV 工程师类比
*   **训练 ResNet-152:** 典型的 Compute Bound。每层卷积都有大量的乘加运算 (MACs)。
*   **瓶颈:** 你的显卡 TFLOPS (每秒浮点运算次数) 不够高。换 4090 比 3090 快，是因为 CUDA Core 更多了。

### 2. Memory Bound (访存密集型) 🚚
*   **症状:** GPU 利用率 **波动较大** 或 **长期低 (30%-50%)**，但显存占用可能很高。
*   **原因:** 算术强度低，数据搬运时间 > 计算时间。GPU 核心大部分时间在等 HBM 把数据送过来。
*   **例子:** `LayerNorm`, `Softmax`, `Attention` (尤其是 Decoding 阶段), `Element-wise` 操作 (ReLU, Add)。
*   **优化方向:** 减少显存读写 (FlashAttention), 算子融合 (Kernel Fusion), 量化 (FP8/INT8) **为了减少搬运量**。

#### 📸 CV 工程师类比
*   **图像预处理 (Data Loading):** 如果你的 DataLoader 没写好，GPU 经常显存空闲等 CPU 读取图片。
*   **LLM Decoding:** 生成每一个 Token，都要把几十 GB 的 KV Cache 读一遍，只为了和新的 Token 做一次 Attention。这就像为了吃一口苹果 🍎，把整棵树都摇了一遍。

## 📊 总结对比表 (Cheat Sheet)

| 特性 | Compute Bound (算力受限) | Memory Bound (带宽受限) |
| :--- | :--- | :--- |
| **典型场景** | ResNet/BERT Training, LLM Prefill (部分) | LLM Decoding, LayerNorm, Activation |
| **GPU 利用率** | 接近 100% | 波动 / 较低 |
| **瓶颈硬件** | ALU / Tensor Cores | HBM Bandwidth / L2 Cache |
| **优化手段** | 提升算力, 量化 (降 MACs) | **FlashAttention**, Kernel Fusion, 量化 (降 IO) |

## ⚔️ 课后实战 (Action Items)
1.  **观察:** 跑一个简单的 ResNet 训练和一个 Llama 推理。
    *   使用 `nvidia-smi -l 1` 观察 GPU Utilization。
    *   ResNet 应该是稳稳的 99-100%。 Llama 推理 (batch=1) 可能会在 30-50% 之间跳动。
2.  **思考:** 为什么说 "量化 (Quantization)" 既能加速 Compute Bound 也能加速 Memory Bound 任务？
    *   (提示: 一个减少了 MACs 计算量，一个减少了 Bytes 搬运量)。

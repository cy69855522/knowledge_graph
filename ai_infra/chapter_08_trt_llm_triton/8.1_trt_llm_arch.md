# 8.1 TRT-LLM Architecture: 以前端之名，行后端之实 🎭

## 🎯 核心目标
TensorRT-LLM (TRT-LLM) 是目前 NVIDIA 官方主推的 LLM 推理库。
这一节我们要搞清楚它的**双层架构**：为什么我们用 Python 写模型，最后跑的却是 C++？

## 💡 概念解析

### 1. The Python Frontend (前端: 只是个画图的) 🎨
在 TRT-LLM 里，你写的 Python 代码 (e.g., `tensorrt_llm.Network`) **并不是在运行推理**，而是在**定义计算图**。

*   **Graph Construction:** 当你运行这段 Python 代码时，它会调用 TensorRT 的 API，在内存里构建一个网络结构 (Network Definition)。
*   **Weight Binding:** 它会把你加载的 PyTorch 权重 (Checkpoint) 绑定到这个图的节点上。
*   **Compilation:** 最后，它调用 `builder.build_engine()`，把这个图编译成一个二进制文件：**TensorRT Engine (.engine)**。

#### 📸 CV 工程师类比
*   **Keras / TensorFlow 1.x:**
    *   **PyTorch (Eager Mode):** 就像现在的 PyTorch。写 `c = a + b`，它马上就算出 c 的值。
    *   **TRT-LLM (Graph Mode):** 就像当年的 **TF 1.x** 或者 **Keras functional API**。
        *   你写 `x = Input(shape=(...))`
        *   `y = Dense(64)(x)`
        *   这时候 `y` 只是一个符号 (Symbol)，里面没有数据。
        *   只有当你 `model.compile()` 并 `model.fit()` 的时候，数据才真正流进去。

### 2. The C++ Runtime (后端: 真正的干活人) 👷‍♂️
一旦 Engine 构建完成，Python 前端的任务就结束了。
推理阶段 (Inference Phase) 是由 **C++ Runtime** 接管的 (虽然外面可能包了一层 Python binding)。

*   **Plugins (插件):** Transformer 里有很多操作 (如 FlashAttention, RoPE, ALL-Reduce) 是标准 TensorRT 算子不支持或者效率不够高的。TRT-LLM 写了大量的 **Custom Plugins (C++ CUDA Kernels)** 来加速这些关键步骤。
*   **MPI / NCCL:** 如果是多卡推理 (Tensor Parallelism)，C++ Runtime 会自动启动多个进程，通过 NCCL 进行通信。

### 3. Workflow Summary (工作流总结) 🔄
1.  **Definition:** Python 代码定义网络结构 (类似搭乐高图纸)。
2.  **Build:** TRT 编译器将图纸 + 权重 -> 编译成二进制 Engine (类似 3D 打印出来的实物)。
3.  **Run:** C++ Runtime 加载 Engine，喂入输入数据，吐出 output。

## ⚔️ 课后实战 (Action Items)
1.  **Inspect:** 去看一个 TRT-LLM 的 example (如 `examples/llama/build.py`)。
    *   找到 `trt_llm.Builder` 和 `network.plugin_config`。
    *   这就相当于你在告诉编译器："我要造一个 Llama，请开启 FlashAttention 插件"。
2.  **Think:** 为什么不直接用 C++ 写网络定义？
    *   (提示: 也就 NVIDIA 能写得动。Python 极其灵活，方便适配各种魔改的模型结构，比如 Llama, Baichuan, Qwen 等等。用 Python 生成 C++ Engine 是目前的最优解)。

# 8.3 Deployment Workflow: ä»é›¶åˆ°ä¸€çš„å®æˆ˜æµç¨‹ ğŸ›£ï¸

## ğŸ¯ æ ¸å¿ƒç›®æ ‡
æˆ‘ä»¬å·²ç»ç†è§£äº† TRT-LLM å’Œ Triton çš„æ¶æ„ï¼Œè¿™èŠ‚æˆ‘ä»¬å°±æŠŠå®ƒä»¬ä¸²èµ·æ¥ï¼Œèµ°ä¸€éå®Œæ•´çš„éƒ¨ç½²æµç¨‹ã€‚
è¿™å« **End-to-End Pipeline**ã€‚

## ğŸ’¡ æ¦‚å¿µè§£æ

### Phase 1: Convert Checkpoint (PyTorch -> TRT-LLM) ğŸ“¦
PyTorch çš„ Checkpoint (e.g., HuggingFace ä¸Šçš„ `.bin` / `.safetensors`) å¿…é¡»å…ˆè½¬æ¢æˆ TensorRT-LLM èƒ½è¯†åˆ«çš„æ ¼å¼ã€‚
*   **Why:** PyTorch å­˜çš„æ˜¯ float32/bfloat16 çš„ raw å‚æ•°ã€‚TRT-LLM éœ€è¦åœ¨è¿™ä¸ªé˜¶æ®µ**æŠŠçŸ©é˜µé‡åŒ–é¢„å¤„ç†å¥½** (æ¯”å¦‚ AWQ çš„ Scale)ã€‚
*   **Command:** é€šå¸¸æ˜¯ `python convert_checkpoint.py ...`ã€‚

### Phase 2: Build Engine (Compile Graph) ğŸ”¨
è¿™æ˜¯æœ€å…³é”®çš„ä¸€æ­¥ã€‚
*   **Input:** ä¸Šä¸€æ­¥è½¬å¥½çš„ Checkpoint + `config.pbtxt` (å®šä¹‰äº†ç½‘ç»œç»“æ„)ã€‚
*   **Process:** **Profile** (åˆ†ææ˜¾å¡æ€§èƒ½) -> **Timing** (è¯•è·‘å„ç§ Kernel) -> **Select** (é€‰å‡ºæœ€å¿«çš„) -> **Serialize** (ç”Ÿæˆ Engine)ã€‚
*   **Output:** `.engine` æ–‡ä»¶ã€‚å®ƒåªè®¤ä½ è¿™å—æ˜¾å¡ (e.g., A100)ï¼Œæ¢åˆ° A10 å°±ä¸è®¤è¯†äº†ã€‚
*   **CV ç±»æ¯”:**
    *   **Phase 1:** å°±å¥½æ¯”æŠŠæºä»£ç  (PyTorch) é¢„å¤„ç†æˆä¸­é—´ä»£ç  (Object File)ã€‚
    *   **Phase 2:** å°±å¥½æ¯” **Linking (é“¾æ¥)**ã€‚æ ¹æ®è¿™ä¸€å°æœºå™¨çš„æŒ‡ä»¤é›† (CUDA SM Architecture)ï¼Œç”Ÿæˆæœ€åçš„å¯æ‰§è¡Œæ–‡ä»¶ (`.exe` / `.engine`)ã€‚

### Phase 3: Triton Serve (Deploy) ğŸš€
æŠŠ Engine å’Œ Config æ–‡ä»¶æŒ‰ç…§ Triton çš„ç›®å½•ç»“æ„æ”¾å¥½ã€‚
*   **Run:** `tritonserver --model-repository=/models ...`
*   **Check:** è®¿é—® `http://localhost:8000/v2/health/ready` ç¡®ä¿æœåŠ¡ç»¿ç¯ã€‚

### Phase 4: Client Request (Inference) ğŸ“²
å®¢æˆ·ç«¯æ€ä¹ˆå‘è¯·æ±‚ï¼Ÿ
*   **gRPC / HTTP:** Triton æä¾›äº†æ ‡å‡†çš„ APIã€‚
*   **Inflight Batching:** ä½ å‘é€çš„è¯·æ±‚ä¼šè¢« Triton æŠŠæ‰€æœ‰ Input æ‹¼åœ¨ä¸€èµ·ï¼Œé€ç»™ TRT-LLMã€‚
    *   TRT-LLM ä¼šæ ¹æ® **Iteration-level Scheduling** (è¿˜è®°å¾— Continuous Batching å—ï¼Ÿ) æ¥è°ƒåº¦ã€‚
    *   å¦‚æœæœ‰è¯·æ±‚å…ˆç»“æŸäº†ï¼Œç»“æœä¼šç«‹åˆ»è¿”å›ç»™ Tritonï¼ŒTriton å†ç«‹åˆ»è¿”å›ç»™ä½ ã€‚

## âš”ï¸ è¯¾åå®æˆ˜ (Action Items)
1.  **GenAI-Perf (Benchmark):** è¿™æ˜¯ä¸€ä¸ªä¸“é—¨æµ‹ LLM æ€§èƒ½çš„å·¥å…·ã€‚
    *   `genai-perf --model llama --concurrency 10 --input-sequence-len 128 --output-sequence-len 128`
    *   å®ƒä¼šç»™ä½ ç”»å‡ºæ¼‚äº®çš„ **Throughput vs Latency** æ›²çº¿ã€‚
    *   è¿™å°±æ˜¯ä½ èƒ½ç»™è€æ¿çœ‹çš„æœ€ç»ˆ Deliverableï¼ğŸ“Š

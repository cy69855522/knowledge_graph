# 8.2 Triton Inference Server: 模型的管家 👮‍♂️

## 🎯 核心目标
模型跑得快 (Inference Latency 低) 只是第一步。
在真实生产环境里，你需要对模型进行 **服务化 (Serving)**：
*   怎么同时处理 1000 个人的请求？
*   怎么让 CPU 和 GPU 并行工作？
*   怎么管理模型的版本？

**Triton Inference Server** 解决了这些问题。它不在乎你用什么框架 (PyTorch, TensorFlow, TensorRT, ONNX)，它只负责 **调度和管理**。

## 💡 概念解析

### 1. Model Repository (模型仓库) 🗂️
Triton 有一套极其严格的目录结构规范，你必须遵守：

```text
model_repository/
  ensemble_model/
    config.pbtxt
    1/
  tensorrt_llm/
    config.pbtxt
    1/
      model.engine
  preprocessing/
    config.pbtxt
    1/
      model.py
```

*   **Version Control:** 每个数字文件夹就是一个版本。你可以轻松回滚到旧版。
*   **Decoupled Components:** 在 LLM 中，通常会把 Preprocessing (Python), TRT Engine (C++), Postprocessing (Python) 拆分成三个独立的 Model。
*   **Ensemble:** 用一个 `ensemble_model` 把这三个串起来 (DAG)。

### 2. Dynamic Batching (动态批处理) 🚌
这是 Triton 最强大的功能之一。
*   **Problem:** 如果每个请求来了马上就送给 GPU 算 (Batch Size = 1)，GPU 只有 10% 的利用率。
*   **Solution:** Triton 设置一个很短的等待窗口 (e.g., 5ms)。
    *   在这 5ms 内到达的所有请求，会被自动拼接成一个 Batch (BS=32)。
    *   一起送给 GPU 计算。
    *   利用率从 10% -> 90%。Latency 只增加了 5ms。

#### 📸 CV 工程师类比
*   **Airport Shuttle:**
    *   **Naive Serving:** 就像打车 (Taxi)。来一个人走一辆车。虽然不用等，但路上全是空车，这叫 **Latency Sensitive**。
    *   **Dynamic Batching:** 就像机场大巴 (Shuttle)。
        *   司机说："我再等 5 分钟，凑满一车再走"。
        *   大家稍微等了一下，但一次运了很多人，平均路费便宜了，这叫 **Throughput Oriented**。

### 3. Concurrent Model Execution (多模型并发) 🤹
Triton 支持在同一个 GPU 上同时跑多个模型实例 (Instance)。
*   **Time Slicing:** 如果显存够大，可以起两个 Llama-7B 实例。Triton 可能会在这一秒跑 Instance 1，下一秒跑 Instance 2，或者利用 CUDA Streams 并行跑。

## ⚔️ 课后实战 (Action Items)
1.  **Read Config:** 去看一个 Triton 的 `config.pbtxt`。
    *   关注 `max_batch_size` 和 `dynamic_batching { preferred_batch_size: [16, 32] }`。
2.  **Monitor:** 启动 Triton 后访问 `http://localhost:8002/metrics`。
    *   你会看到 `queue_time` (排队时间) 和 `compute_input_time` (计算时间) 的监控指标。这是做性能调优的金矿。

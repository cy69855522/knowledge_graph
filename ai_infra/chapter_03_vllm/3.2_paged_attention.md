# 3.2 PagedAttention: vLLM 的核心魔法 🪄

## 🎯 核心目标
为什么 vLLM 能比 HuggingFace Transformers 快 10 倍？秘密就在这里：**终结显存碎片 (Fragmentation)** 🧱。

## 💡 概念解析

### 1. 显存碎片 (Fragmentation) 💥
*   **问题:** 传统 HuggingFace 实现，因为我们不知道生成的句子有多长，所以必须预分配一个最大长度 (Max Length, e.g., 2048) 的连续显存块。
*   **后果:** 用户只问了 10 个字，你却占了 2048 个字的坑。这就是 **Internal Fragmentation (内部碎片)**。显存全是空的，但不能给别人用。

#### 📸 CV 工程师类比
*   **Static Tensor Allocation:** 就像你声明了一个 `Tensor[Batch, 2048, Hidden]`，但实际只用了前 50 个位置。剩下的全是 0。
*   **VS PagedAttention:** 想象你在做 **Texture Atlas (纹理图集)** 打包 📦。不再需要申请一张巨大的空白图，而是把小纹理塞到显存里的任意空隙中。

### 2. PagedAttention 机制 📖
vLLM 引入了操作系统中 **虚拟内存 (Virtual Memory) & 分页 (Paging)** 的概念。
*   **Block:** 将 KV Cache 切分成固定大小的小块 (Blocks, e.g., 16 tokens/block)。
*   **Block Table:** 维护一个映射表 (类似 Page Table)，记录逻辑 block 到物理 block 的地址。
*   **非连续存储:** 物理上可以是乱序的，甚至是跨 GPU 的。

### 3. Copy-on-Write (写时复制) for Beam Search 👯‍♀️
在做 Beam Search (生成多个候选句子) 时，多个候选句子可能共享同一个前缀。
*   **PagedAttention:** 不需要复制前缀的 Block，只需要让多个 Block Table 指向同一个物理 Block。只有当某个句子生成了不一样的后缀时，才申请新的 Block。
*   **这极大地节省了显存！**

### 4. Block Size 的权衡 (Trade-off) ⚖️
Block Size (页大小) 是 vLLM 中最重要的可配置参数，通常默认为 **16** 或 **32**。

#### 🔹 如果设置过小 (e.g., 1)
*   **优点:** **Internal Fragmentation** 几乎为零。
*   **缺点:** **Metadata Overhead** 激增。Block Table 会变得巨大，且 GPU 访存变得极其细碎，无法利用内存带宽，导致计算变慢。

#### 🔹 如果设置过大 (e.g., 1024)
*   **优点:** Metadata 很少，访存连续。
*   **缺点:** **Internal Fragmentation** 再次出现。显存中会有大量“已分配但未被使用”的空洞，导致显存被浪费，显著降低 **Max Batch Size**。

## ⚔️ 课后实战 (Action Items)
1.  **Read Code:** 去 vLLM 源码里搜 `BlockTable` 这个类。看看它是怎么管理 block 索引的。
2.  **思考:** 既然内存不连续了，Attention 计算的时候怎么访存？(提示：需要专门写的 CUDA Kernel，也就是 `paged_attention_kernel`，去查表拿数据)。

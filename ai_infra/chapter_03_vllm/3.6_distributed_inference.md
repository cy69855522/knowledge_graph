# 3.6 Distributed Inference: 众人拾柴火焰高 🔥

## 🎯 核心目标
模型太大，显卡装不下怎么办？70B 的 Llama 需要 140GB+ 显存 (FP16)，但单卡最多只有 80GB。我们必须切分模型 (Model Partitioning)。

## 💡 概念解析

### 1. Tensor Parallelism (TP) - 张量并行 🧱
把一个巨大的计算矩阵，切分到多张卡上并行计算。
*   **Row Parallel:** 按行切分 $W$ 矩阵。
*   **Column Parallel:** 按列切分 $W$ 矩阵。
*   **All-Reduce:** 每算完一层 Linear Layer，必须让所有卡同步结果。通信量极大，需要 NVLink。

#### 📸 CV 工程师类比
*   **Split Width / Height:**
    *   想象你要训练一个超宽的 ResNet (比如 channel=1024)。
    *   **TP:** 你把 channel=1024 切成两半，卡 0 算前 512 个通道，卡 1 算后 512 个通道。
    *   **Sync:** 在做 BatchNorm 或下一步卷积前，必须先把两张卡的中间特征图同步加起来。

### 2. Pipeline Parallelism (PP) - 流水线并行 🚇
把模型的不同层 (Layers) 放到不同的卡上。
*   **Stage 1 (GPU 0):** Layer 0-15
*   **Stage 2 (GPU 1):** Layer 16-31
*   **Bubble (气泡):** 为了让 GPU 1 起来干活，GPU 0 计算完必须把数据传给 GPU 1。如果没有优秀的调度，显卡会有很多闲置时间 (Bubble)。

#### 📸 CV 工程师类比
*   **Production Line:**
    *   汽车流水线。
    *   **GPU 0 (冲压):** 负责把数据预处理、Embedding。
    *   **GPU 1 (涂装):** 负责 Transformer Block 计算。
    *   **GPU 2 (总装):** 负责最后的 LM Head 输出。
    *   **特点:** 第一辆车还没走到最后，第一台机器就可以开始处理第二辆车了。

## ⚔️ 课后实战 (Action Items)
1.  **Thinking:** 为什么推理时 (Inference) 主要用 TP，很少用 PP？
    *   (提示: Latency。PP 会因为跨卡传输和等待，显著增加单个 token 的生成延迟。只有当模型大到 TP 都切不动了 (比如万亿参数 MoE)，才会考虑 PP)。
2.  **Debug:** 遇到 `NCCL timeout` 怎么查？
    *   (提示: 多卡通信死锁，或者某张卡掉线了。check `nvidia-smi` P2P status)。

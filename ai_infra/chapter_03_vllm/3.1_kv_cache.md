# 3.1 KV Cache (Key-Value Cache) 🔑

## 🎯 核心目标
为什么大模型推理这么吃显存？因为我们要存 **KV Cache**。

## 💡 概念解析

### 1. 什么是 KV Cache? 🤔
在 Transformer 的自回归 (Auto-regressive) 生成过程中，为了避免重复计算已经生成过的 token 的 Key 和 Value 矩阵，我们将它们缓存下来。
Attention 机制本身是 O(N^2) 的，但因为是 Causal (只能看过去)，我们可以复用之前的计算结果。

### 2. 推理的两大阶段 (The Two Phases) ✌️
LLM 推理并不是从头到尾都一样的，它分为两个截然不同的阶段：

#### 🟢 Phase 1: Prefill (预填充 / Prompt Processing)
*   **What:** 处理用户输入的 Prompt (提示词) 的阶段。
*   **Action:** 一次性并行计算所有 Prompt token 的 KV Cache。
*   **特点 (Compute Bound):** 因为是并行矩阵乘法 (GEMM)，且一次性处理很多 token，计算密度高，主要受限于 GPU 计算能力 (TFLOPS)。
*   **Latency:** 也就是 **Time to First Token (TTFT)**。

#### 🔵 Phase 2: Decode (解码 / Token Generation)
*   **What:** 一个接一个生成新 token 的阶段。
*   **Action:** 每次只生成 1 个 token，依赖上一步的 KV Cache。
*   **特点 (Memory Bound):** 每次都要搬运巨大的 KV Cache 到计算单元，但只计算这 1 个 token。主要受限于 **显存带宽 (Memory Bandwidth)**。
*   **Latency:** 也就是 **Time Per Output Token (TPOT)**。

### 3. 为什么需要它? (The Why)
如果不缓存，每生成一个新词，都要把之前所有的词重新算一遍 Attention。
*   生成第 1000 个词：计算量 1000。
*   生成第 1001 个词：计算量 1001。
*   总复杂度: O(N^2)。慢得像乌龟 🐢。

如果缓存了 (KV Cache)：
*   生成第 1000 个词：只算这 1 个新词的和前面 999 个 cached K/V 的 Attention。
*   总复杂度: O(N)。飞快 🐇。

#### 📸 CV 工程师类比
*   **Video Object Tracking (视频目标跟踪):**
    *   **Naive 方法:** 每来一帧新图像，你都把 *从第一帧到当前帧* 的所有图像重新输入 CNN 提取特征。 -> 显卡冒烟 💥。
    *   **KV Cache 方法:** 你只计算 *新的一帧* 的特征，并复用之前保存下来的特征图 (Feature Maps) 更新状态 (Hidden State)。
    *   **区别:** CV 的特征图大小通常固定 (如 `H x W`)，但在 LLM 中，随着生成 token 增加，KV Cache 的**序列长度 (Sequence Length) 是不断增长的** 📈。

## ⚔️ 课后实战 (Action Items)
1.  **算一算:** Llama-2-7B (float16) 存一个 token 的 KV Cache 需要多少显存？
    *   公式: `2 (K+V) * n_layers * n_heads * head_dim * sizeof(float16)`
    *   Llama-2-7B: 32 layers, 32 heads, 128 dim.
    *   `2 * 32 * 32 * 128 * 2 bytes = 524,288 bytes` ≈ **0.5 MB / token**。
    *   如果你有 2048 context length，一个用户就要吃掉 1GB 显存！User * Batch，显存瞬间爆炸 💥。

### 4. 常见疑问：为什么不把 KV Cache 放在寄存器 (Register) 里？ 🤔
这是一个非常好的计算机体系结构问题！答案很简单：**放不下**。

让我们看一组数据对比 (以 NVIDIA A100 80GB 为例)：

*   **Register File 大小:**
    *   每个 SM (Streaming Multiprocessor) 有 256 KB 寄存器堆。
    *   A100 有 108 个 SM。
    *   **全卡总寄存器容量:** `108 * 256 KB ≈ 27 MB`。
*   **KV Cache 大小:**
    *   如上计算，1 个用户的 sequence length 达到 2048 时，KV Cache 大小约为 **1 GB**。
    *   `1 GB (KV Cache) >> 27 MB (Total Registers)`。

**结论:** 寄存器虽然速度最快 (比 HBM 快 100 倍以上)，但容量极小，只能存放当前正在计算的几个数 (Operands)。KV Cache 这种海量数据只能存放在容量最大的 **HBM (High Bandwidth Memory)** 中，这就是为什么 LLM 推理是 **Memory Bound** 的根本原因。

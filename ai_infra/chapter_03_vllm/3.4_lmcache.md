# 3.4 LMcache: 拒绝重复劳动 🚫

## 🎯 核心目标
解决多轮对话 (Multi-turn conversation) 或 RAG (检索增强生成) 中，重复计算 Prompt KV Cache 的问题。

## 💡 概念解析

### 1. Prefill Latency 🐢
每次用户发来几千字的文档 (Context) 做总结或问答。
如果不做优化，vLLM 必须把这几千字的文档重新算一遍 KV Cache (Prefill 阶段)。这非常慢，且浪费算力。

### 2. LMcache (KV Cache Offloading) 💾
核心思想：**把算好的 KV Cache 存起来**。
*   **Layer 1 (GPU Memory):** 最热的数据留在显存。
*   **Layer 2 (CPU Memory):** 次热的数据卸载到内存。
*   **Layer 3 (Disk/Network):** 冷数据存盘或者传给其他机器。

#### 📸 CV 工程师类比
*   **Video Decoder Cache:**
    *   你看 B 站视频回退进度条 ⏪。
    *   不需要重新从头解码前面的 GOP (Group of Pictures)。
    *   直接从内存读取之前的 **解码帧 (Decoded Frame)**。
    *   如果内存不够了，就丢弃最老的帧 (LRU)。

### 3. Prefix Caching (前缀缓存) 🌲
如果多个请求共享同一个 System Prompt (比如 "你是一个有用的助手...")。
vLLM 会识别到这个公共前缀，直接复用它的 KV Cache Block，不需要重复计算。

## ⚔️ 课后实战 (Action Items)
1.  **Thinking:** 为什么 LMcache 在 RAG (Retrieval Augmented Generation) 场景下是神器？
    *   (提示: 很多用户的 Query 不同，但 retrieved documents 往往是重复的。复用 docs 的 KV Cache 能极大降低 TTFT)。
2.  **Architecture:** 思考一个分布式的 LMcache 系统。如果我在机器 A 算过 KV Cache，机器 B 能直接读吗？(需要 RDMA 或者快速网络存储)。

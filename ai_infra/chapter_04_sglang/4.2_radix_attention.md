# 4.2 RadixAttention (基数注意力): 把 KV Cache 变成树 🌳

## 🎯 核心目标
为什么 vLLM 的 Prefix Caching 还是不够极致？因为它是**基于哈希 (Hash)** 的。如果 prompt 哪怕变了一个字，Hash 就全变了，缓存无法命中。
SGLang 引入了 **RadixAttention**，将整个 KV Cache 变成了一棵**前缀树 (Radix Tree)**。

## 💡 概念解析

### 1. 传统 Prefix Caching 的局限 🚧
vLLM 的 Prefix Caching 通常是 Block Level 的。
*   如果你的 System Prompt 正好占满了 3 个 Block，这部分 KV Cache 能被复用。
*   但如果你的 Prompt 稍微改了一点 (Append 了一些新 Token)，或者你想**回滚 (Rollback)** 到之前的某个状态继续生成，vLLM 很难找到之前的缓存。

### 2. RadixAttention 的魔法 ✨
SGLang 不把 KV Cache 只看作是一个扁平的列表，而是**全自动**地维护一棵树。每个节点代表一段 Token 的 KV Cache。
*   **Insert:** 每当你发一个请求，它的 Prompt 如果是新的，就在树上分叉出一个新枝。
*   **Match:** 如果你的 Prompt 只是比树上的某个节点多了一段后缀，它会**最长前缀匹配 (Longest Prefix Match)**，直接复用已有节点的 KV Cache。
*   **Evict (LRU):** 如果显存不够了，从叶子节点开始，删掉那个最久没访问的分支 (Leaf)。

#### 📸 CV 工程师类比
*   **Git / Version Control:**
    *   **vLLM:** 就像每次 Commit 其实都是全量备份，如果文件没变就存个指针。但一旦你想 `checkout` 到之前某个 commit 并在那里新开一个 branch，vLLM 的状态管理就乱了。
    *   **SGLang:** 就像真正的 **Git 图结构 (DAG)**。
        *   每个 Commit 都是树上的一个节点。
        *   你可以随时 `fork` (多路生成)。
        *   你可以随时 `reset` (回滚生成)。
        *   而且所有相同的前缀路径，**物理上只存一份 KV Cache**，绝对不浪费显存。

### 3. 应用场景 🎬
*   **Few-shot Learning:** 固定的例子作为树的根节点。
*   **Self-Consistency (CoT):** 同样的问题，生成 10 个不同的推理路径。这 10 个路径共享问题描述的 KV Cache。
*   **Tree Search (ToT):** 思维树搜索。在某个节点发现不对，回退一步换个方向。RadixAttention 能完美恢复之前的状态，**零延迟**。

## ⚔️ 课后实战 (Action Items)
1.  **Thinking:** 如果我想让多个不同的 System Prompt 共享 KV Cache，可能吗？
    *   (提示: 不行。因为 KV Cache 是依赖前文的。System Prompt 必须完全一致才能复用。但你可以把公共的部分提出来作为公共前缀)。
2.  **Implementation:** SGLang 怎么知道哪些 KV Cache 能够被Evict？
    *   (提示: 引用计数 (Reference Counting)。只要有 Request 还在引用这个节点，或者它的子节点还在活跃，它就不能被删)。

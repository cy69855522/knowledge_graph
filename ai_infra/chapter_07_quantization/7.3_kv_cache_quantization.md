# 7.3 KV Cache Quantization: 解决显存瓶颈的终极杀招 🗡️

## 🎯 核心目标
显存 (HBM) 不够用，除了把模型本身变小 (W4A16)，还能怎么办？
**KV Cache** 是显存占用的另一大巨头，特别是在长 Context 和大 Batch Size 下。
把 **KV Cache 压成 FP8**，显存占用瞬间只要一半，吞吐量翻倍。

## 💡 概念解析

### 1. The Bottleneck: HBM (瓶颈在哪里) 🚧
*   **LLM Inference:** 是 **Memory Bound** (内存带宽受限) 的任务，尤其是 Decode 阶段。
*   **Context Length:** 随着 context 变长，KV Cache 线性增长。
*   **Batch Size:** 随着并发数增加，KV Cache 线性增长。

如果能把 KV Cache 从 FP16 (2 Bytes) 变成 FP8 (1 Byte)，你就多了一倍的显存空间！

### 2. FP8 (E4M3 vs E5M2) 🧬
FP8 有两种标准格式 (OCP/NVIDIA Hopper 提出)：
*   **E4M3 (4位指数，3位尾数):** 动态范围较小，但精度较高。适合存 **权重 (Weights)** 和 **激活值 (Activations)**。
*   **E5M2 (5位指数，2位尾数):** 动态范围较大 (跟 FP16 一样)，但精度极低。适合存 **梯度 (Gradients)**。
*   **KV Cache 用哪个？** 通常用 **E4M3**。

#### 📸 CV 工程师类比
*   **Image Format:**
    *   **FP16:** 就像 **PNG-16**。颜色极度丰富，但文件巨大。
    *   **FP8 (E4M3):** 就像 **GIF (256色)**。虽然只有 256 种颜色，但对于大多数自然图像，肉眼看还是过得去的。

### 3. Implementation (实现细节) 🛠️
*   **Quantize:** 在把 $K, V$ 写入 Cache 之前，先除以 Scale，转成 FP8。
*   **Dequantize:** 在做 Attention 计算的时候，Kernel 内部把 FP8 读出来，乘上 Scale，恢复成 FP16/BF16 再做点积。
*   **Note:** 这需要专门的 **FlashAttention Kernel** 支持 (FP8 input)。

## ⚔️ 课后实战 (Action Items)
1.  **Check vLLM:** 在 vLLM 里开启 KV Cache Quantization 非常简单。
    *   `--kv-cache-dtype fp8`
    *   这就完了。vLLM 底层会自动调用支持 FP8 的 PagedAttention Kernel。
2.  **Trade-off:** 精度损失了吗？
    *   (提示: 对于 70B 以下的模型，几乎无损。在这个 scale 下，模型的鲁棒性很强。但对于极小的模型 (e.g., 1B)，可能会崩)。

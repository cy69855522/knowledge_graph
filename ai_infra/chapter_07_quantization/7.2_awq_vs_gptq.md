# 7.2 AWQ vs GPTQ: 谁才是 W4A16 的王者？👑

## 🎯 核心目标

现在大家都想把模型压到 4-bit (W4A16)，既能省显存，又能加速。
主流选手有两个：**AWQ** 和 **GPTQ**。面试必考它们的区别！

## 💡 概念解析

### 1. GPTQ: The Teacher (全局优化) 👨‍🏫

* **Full Name:** Generative Pre-trained Transformer Quantization.
* **Method:** **OBQ (Optimal Brain Quantization)** 的进化版。它是一个纯数学优化问题。
  * 目标：最小化 $\parallel WX - \hat{W}X \parallel^2$。
  * 手段：逐层重构。量化权重的误差，会通过 **Hessian Matrix (海森矩阵)** 的逆矩阵，补偿给还没有量化的权重。
* **CV 类比:**
  * **Global Optimization (全局优化):** 就像你在做 **Image Denoising (图像去噪)**。你有一个能量函数 (Energy Function)，通过全局迭代，让去噪后的图像跟原图最像。不区分这个像素是人脸还是背景。

### 2. AWQ: The Strategist (重点保护) 👮‍♂️

* **Full Name:** Activation-aware Weight Quantization.
* **Insight:** 并不是所有权重都一样重要！
  * 只有 **1% 的权重** (Salient Weight) 对应着很大的激活值 (Large Activation)。
  * 如果把这 1% 的权重保护起来 (保持 FP16)，其他的全压成 INT4，精度几乎不掉。
* **Method:** 不做混合精度 (那是为了部署方便)。而是把 Salient Weight 所在的 Channel **放大 (Scale Up)**，然后再量化。因为量化的绝对误差是固定的 ($\Delta = S/2$)，放大后的相对误差就小了！

#### 📸 CV 工程师类比

* **ROI (Region of Interest) Encoding:**
  * **GPTQ:** 对整张图一视同仁，用同样的压缩率。
  * **AWQ:** 识别出 **人脸区域 (Activation Large)**。给这块区域分配 **两倍的码率 (Scale Up)**，背景区域分配低码率。最后整张图看起来还是高清的。

### 3. The Math Trick: Scale Equivalence (你的直觉是对的) 🧮

正如你所问：**权重放大 $s$ 倍，结果不就变了吗？**
是的！必须要有一个地方把这个 $s$ 除回去。

AWQ 的数学公式利用了**矩阵乘法的交换律**：

$$
Y = W X = (W \cdot s) \cdot (X / s)
$$

* **Offline (离线准备):**
  1. 我们要保护 $W$ 里的某些重要权重，给它乘上 $s$。
  2. 这就得到了一个新的权重矩阵 $W' = W \cdot s$。
  3. 然后把 $W'$ 量化成 INT4。由于 $W'$ 的数值变大了，相对误差变小了，精度得以保留。
* **Runtime (推理时刻):**
  1. 既然 $W$ 变大了，为了让 $W X$ 的结果不变，我们在做矩阵乘法之**前**，必须先把输入激活值 $X$ **缩小 $s$ 倍**。
  2. 即：计算 $X' = X / s$。
  3. 最后计算 INT4 GEMM：$Y \approx \text{Quant}(W') \times X'$。

**巧妙之处：**
这个除法操作 ($1/s$) 通常可以被 **融合 (Fused)** 到前面的层 (比如 LayerNorm/RMSNorm) 里。
所以推理的时候，几乎是 **零代价 (Zero Overhead)** 的！你不需要以此为代价牺牲速度，却白捡了精度。

### 3.1 Deep Dive: How Fusion Works (RMSNorm Example) 🔬

你可能会问：**"RMSNorm 也是加权求和吗？"**
并不是。RMSNorm (Root Mean Square Normalization) 的公式是：

$$
y = \frac{x}{\text{RMS}(x)} \cdot \gamma
$$

其中 $\gamma$ (Gamma) 是一个**可学习的缩放参数**。就是去掉减均值操作的layernorm

**AWQ 的融合过程：**
我们要计算的是 $X' = X / s$。而 $X$ 正是 RMSNorm 的输出。

$$
X' = (\frac{\text{Input}}{\text{RMS}} \cdot \gamma) / s = \frac{\text{Input}}{\text{RMS}} \cdot (\frac{\gamma}{s})
$$

**Fusion Trick:**
我们不需要在推理时多算一步除法。只需要在离线量化的时候，直接修改模型权重，把 RMSNorm 里的 $\gamma$ 参数变成 $\gamma' = \gamma / s$ 即可！
这就是为什么说是“零代价”。

### 3.2 FAQ: 既然放大能保精度，为什么不把所有 Channel 都放大？ 🤔
这也是一个极好的直觉问题！
你猜到了吗？**不是因为计算开销 (那是融合掉的)**，真正的阻碍是 **量化分组 (Quantization Group)**。

*   **Group Size:** 通常每 128 个权重 (Weight Block) 共享**一个**量化比例尺 (Scale)。这 128 个权重来自**不同的输入通道**。
*   **The Trap (陷阱):**
    *   如果你放大了一个**不重要**的通道 (Activation 很小)，它所对应的权重值 $W$ 会变得很大。
    *   这会导致整个 Group 的 **动态范围 (Dynamic Range)** 被撑大。
    *   为了容纳这个变大的值，Group 的 **量化步长 (Step Size)** 必须变大 -> **量化噪声 (Noise)** 变大。
    *   **结果:** 这个 Group 里其他**真正重要**的权重，因为步长变大了，精度反而下降了！得不偿失。

**结论:** 我们只能通过 Grid Search 找到一个**平衡点**：只放大那些**收益 (激活值贡献)** 大于 **代价 (Group 步长增加)** 的通道。

### 3.3 FAQ: 为什么不每个通道(甚至每个权重)一个比例尺？ 📏
问得好！这里是 **量化 (Quantization)** 的核心权衡：**Overhead vs. Precision**。

*   **Scenario A: Per-Weight Quantization**
    *   设想每一个 4-bit 权重，都配一个 FP16 (16-bit) 的 Scale。
    *   **Data:** 4 bits.
    *   **Overhead:** 16 bits.
    *   **Total:** 20 bits.
    *   **Result:** 比 FP16 (16 bits) 还大！量化了个寂寞。😂

*   **Scenario B: Per-Channel Quantization (Group Size = Hidden Dim)**
    *   一整列权重 (比如 4096 个) 共享一个 Scale。
    *   **Overhead:** 忽略不计。
    *   **Problem:** 这一列里只要有一个离群值，整列 4096 个数的精度都被拖垮了。

*   **Scenario C: Group Quantization (Group Size = 128)**
    *   折中方案。每 128 个数共享一个 Scale。
    *   **Overhead:** $16 / (128 \times 4) \approx 3\%$。可以接受。
    *   **Precision:** 局部性很好，离群值只会影响它附近的 128 个数，不会波及全局。
    *   **Hardware:** 128 正好是 CUDA Warp (32 threads x 4) 的倍数，读取效率极高。

### 4. Comparison (对比总结) ⚔️

| Feature        | GPTQ                                         | AWQ                                      |
| :------------- | :------------------------------------------- | :--------------------------------------- |
| **原理** | 误差补偿 (Error Compensation)                | 激活感知 (Activation Protection)         |
| **依赖** | 需要 Calibration Data (校准集) 算 Hessian    | 需要 Calibration Data 找 Salient Weights |
| **速度** | 量化过程慢 (需要求逆矩阵)，但在 GPU 上还可以 | 量化过程极快 (只是统计一下激活值)        |
| **精度** | 在极低比特 (2-bit/3-bit) 下表现更好          | 在 4-bit 下泛化性更好 (不易过拟合校准集) |
| **部署** | 早期支持好 (AutoGPTQ)                        | 现在 vLLM 原生支持最好                   |

## ⚔️ 课后实战 (Action Items)

1. **Thinking:** 为什么 AWQ 不直接把重要的权重留成 FP16，非要搞什么 Scaling？
   * (提示: 硬件友好性。如果一个 Layer 里面既有 INT4 又有 FP16，CUDA Kernel 就很难写，Warp Divergence 会很严重。AWQ 让所有权重都是 INT4，只是通过 Scaling Trick 变相提高了精度)。
2. **Coding:** 用 `autoawq` 库量化一个 Llama-2-7B。
   * 观察它在量化过程中打印的 "Scaling factors"。你会发现某些 Channel 的缩放比例特别大。

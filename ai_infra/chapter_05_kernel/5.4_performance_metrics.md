# 5.4 Performance Metrics: 屋顶模型 (Roofline) 📈

## 🎯 核心目标
我的算子跑得慢，到底是显卡不行，还是我代码不行？
**Roofline Model** 用一张图告诉你，你的性能瓶颈究竟在哪里。

## 💡 概念解析

### 1. 两个金标准 🥇
*   **Peak TFLOPS (算力峰值):** 显卡每秒最多能算多少次浮点运算。
    *   *A100 (FP16):* 312 TFLOPS (Tensor Core)。
*   **Peak Bandwidth (带宽峰值):** 显卡每秒最多能搬运多少 GB 数据。
    *   *A100 (HBM2e):* 2,039 GB/s。

### 2. Roofline Model (屋顶模型) 🏠
我们把 **Operational Intensity (运算强度)** 定义为 $I = \frac{\text{FLOPs}}{\text{Bytes}}$。
即：每搬运 1 Byte 的数据，你需要做多少次浮点计算。

*   **Memory Bound (带宽受限区):**
    *   $I$ 很小 (如 Element-wise Add, Matrix-Vector Mul, Decode)。
    *   你的算子性能受限于 **带宽**。
    *   性能 = Bandwidth $\times$ Intensity。
    *   *Optimization Strategy:* 省 IO。减少 HBM 读写，用 FlashAttention。

*   **Compute Bound (算力受限区):**
    *   $I$ 很大 (如 Matrix-Matrix Mul, Conv, Prefill)。
    *   你的算子性能受限于 **TFLOPS**。
    *   性能 = Peak TFLOPS。
    *   *Optimization Strategy:* 用 Tensor Core，写好 Pipeline，把计算单元塞满。

#### 📸 CV 工程师类比
*   **Production Line:**
    *   **Memory Bound:** 就像 **物流中心**。卡车来得不够快，里面的工人都在等货。再雇一百个肌肉男也没用 (TFLOPS 没用)，得加更多卸货口 (Bandwidth)。
    *   **Compute Bound:** 就像 **精密加工厂**。货堆得满地都是，工人根本做不过来。这时候得换全自动机床 (Tensor Core)。

### 3. Latency vs Throughput ⏱️
*   **Latency (延迟):** 一个请求多久回来。对在线推理 (Online Inference) 至关重要。
*   **Throughput (吞吐量):** 一秒能处理多少个请求。对离线批处理 (Offline Batch) 至关重要。
*   **Little's Law:** $L = \lambda W$。Latency 和 Throughput 往往是矛盾的 (Batch Size 越大，Throughput 越高，但 Latency 也跟着涨)。

## ⚔️ 课后实战 (Action Items)
1.  **Profiling:** 随便跑一个 PyTorch 模型，用 `nsys profile` 抓一下。
    *   看看那些 Kernel 的 SM 利用率是多少？DRAM 利用率是多少？
    *   如果 DRAM 100% 而 SM 10% -> Memory Bound。
2.  **Calculation:** 算一下 A100 的 Roofline 转折点 (Turning Point)。
    *   $I_{turn} = \frac{312 \text{ TFLOPS}}{2 \text{ TB/s}} \approx 150 \text{ FLOPs/Byte}$.
    *   如果你的算子每读一个字节做不到 150 次运算，那你永远别想跑满算力峰值。

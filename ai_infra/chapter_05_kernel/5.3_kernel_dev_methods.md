# 5.3 Kernel Development Methods: 屠龙刀 vs 倚天剑 ⚔️

## 🎯 核心目标
怎么写出一个高性能的 AI 算子 (Kernel)？
以前只有 **CUDA C++** 这一条路。现在 **OpenAI Triton** 给了 Python 开发者一条捷径。

## 💡 概念解析

### 1. CUDA C++: The Original Way 🛠️
*   **语言:** C++。
*   **控制:** 极其精细。你需要手动管理：
    *   HBM -> SRAM 的数据搬运 (`async_copy`)。
    *   SRAM -> Register 的读取。
    *   Tensor Core 的调用 (`wmma::mma_sync` / `mma_m16n8k16`).
    *   线程同步 (`__syncthreads()`).
*   **难度:** High。非常容易写错同步导致结果不准，或者因为 Bank Conflict (Shared Memory 冲突) 导致性能暴跌。

### 2. OpenAI Triton: The Modern Way 🚀
*   **语言:** Python (Looking)。但会被 JIT 编译成机器码。
*   **控制:** 块级语义 (Block Semantics)。
    *   **CUDA:** 你指挥 32 个线程，每个线程搬运 1 个数。
    *   **Triton:** 你指挥 1 个 Block，直接说 `tl.load(PTR_BLOCK)`，把一整块数据搬进 SRAM。
*   **优势:** Triton 编译器自动帮你做流水线优化 (Pipeline Scheduling)、Shared Memory 布局优化、MMA 指令选择。
*   **Result:** 用 1/10 的代码量，就能达到甚至超越手写 CUDA 的性能 (比如 FlashAttention 的 Triton 版本)。

#### 📸 CV 工程师类比
*   **Assembly vs NumPy:**
    *   **CUDA:** 就像你在写 **汇编 (Assembly)** 或 **Intrinsic (SIMD)**。你需要精细地调度每一个寄存器。
    *   **Triton:** 就像你在写 **NumPy/JAX**。
        *   你写的是 `C = A @ B`。
        *   NumPy 底层自动帮你调用 BLAS 库，或者由编译器 (Triton) 帮你把这行代码翻译成高性能的 CUDA 汇编。

### 3. Which one to learn? 🤔
*   **For Product:** 学 Triton。迭代快，易维护，跨平台 (AMD/NVIDIA 都能跑)。
*   **For Performance Geek:** 学 CUDA。如果你要那一丢丢极致的性能，或者你要写极其特殊的算子，Triton 覆盖不了的时候，还是得手写 CUDA。

## ⚔️ 课后实战 (Action Items)
1.  **Coding:** 去 Triton 的 GitHub 看那个简单的 `Vector Add` 和 `Matmul` 教程。
    *   你会发现它非常像普通的 PyTorch 代码，但每一个变量其实代表了一个 Block。
2.  **Benchmark:** 跑一下 Triton Matmul 和 PyTorch `torch.matmul` 的性能对比。
    *   你会发现对于常规矩阵形状，Triton 几乎能打平 cuBLAS。

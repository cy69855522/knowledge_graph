# 2.5 CUDA Graph ğŸ•¸ï¸

## ğŸ¯ æ ¸å¿ƒç›®æ ‡
ç†è§£ä¸ºä»€ä¹ˆ GPU å¾ˆå¿«ï¼Œä½†æœ‰æ—¶å€™ä½ çš„æ¨¡å‹è·‘å¾—æ…¢ã€‚å­¦ä¼šç”¨ **CUDA Graph** æ¶ˆé™¤ "CPU å‘å·æ–½ä»¤" çš„å¼€é”€ (Overhead) ğŸš€ã€‚

## ğŸ’¡ æ¦‚å¿µè§£æ

### 1. Kernel Launch Overhead (å‘å·æ–½ä»¤çš„ä»£ä»·) ğŸ—£ï¸
GPU æ˜¯ä¸ªè¶…å¼ºçš„æ‰“å·¥äºº (Worker)ï¼ŒCPU æ˜¯è€æ¿ (Manager)ã€‚
*   **Kernel Launch:** è€æ¿å‘Šè¯‰æ‰“å·¥äººï¼š"æŠŠè¿™ä¸ª Tensor åŠ é‚£ä¸ª Tensor"ã€‚
*   **Overhead:** è€æ¿è¯´è¯ä¹Ÿæ˜¯è¦æ—¶é—´çš„ï¼(Python overhead + CUDA Driver overhead)ã€‚
*   **é—®é¢˜:** å¦‚æœæ´»å„¿å¾ˆå° (Small Kernels)ï¼Œæ¯”å¦‚åªæ˜¯åšä¸ªåŠ æ³•ï¼Œæ‰“å·¥äºº 1us å¹²å®Œäº†ï¼Œä½†è€æ¿è¯´è¯èŠ±äº† 10usã€‚
    *   **ç»“æœ:** GPU å¤§éƒ¨åˆ†æ—¶é—´åœ¨**ç­‰è€æ¿è¯´è¯** (GPU Starvation / CPU-Bound)ã€‚ğŸ˜°

### 2. CUDA Graph (å¥—é¤åˆ¶åº¦) ğŸ±
CUDA Graph å°±æ˜¯**æŠŠä¸€å †å‘½ä»¤æ‰“åŒ…**ï¼Œä¸€æ¬¡æ€§å‘ç»™ GPUã€‚
*   **Capture (å½•åˆ¶):** è€æ¿å…ˆæ¼”ç¤ºä¸€éï¼Œå¹¶ä¸çœŸçš„æ‰§è¡Œï¼Œè€Œæ˜¯æŠŠæ‰€æœ‰å‘½ä»¤è®°åœ¨ä¸€ä¸ªå°æœ¬æœ¬ä¸Š (Graph)ã€‚
*   **Replay (å›æ”¾):** ä»¥åè€æ¿åªè¦è¯´ï¼š"æ‰§è¡Œç¬¬ 5 å·å¥—é¤"ï¼ŒGPU å°±è‡ªå·±ç…§ç€å°æœ¬æœ¬æŠŠé‚£å‡ åä¸ª Kernel ä¸€å£æ°”è·‘å®Œã€‚
*   **å¥½å¤„:** åªéœ€è¦å‘**ä¸€æ¬¡**å‘½ä»¤ï¼Œå°±èƒ½è·‘**ä¸€å †** Kernelã€‚Launch Overhead å‡ ä¹å½’é›¶ï¼ğŸ“‰

#### ğŸ“¸ CV å·¥ç¨‹å¸ˆç±»æ¯”
*   **Eager Mode (æ—  Graph):** å°±åƒä½ å»é¥­åº—å•ç‚¹ (A la carte)ã€‚
    *   ä½ ï¼š"æœåŠ¡å‘˜ï¼Œæ¥æ¯æ°´ / æœåŠ¡å‘˜ï¼Œæ¥ä¸ªèœå• / æœåŠ¡å‘˜ï¼Œç‚¹ä¸ªå®«ä¿é¸¡ä¸..."
    *   æœåŠ¡å‘˜è·‘æ–­è…¿ï¼Œä½ ä¹Ÿç­‰å¾—æ€¥ã€‚
*   **CUDA Graph:** å°±åƒä½ ç›´æ¥ç‚¹ **"A å¥—é¤"**ã€‚
    *   ä½ ï¼š"A å¥—é¤ï¼" -> å¨æˆ¿ç›´æ¥å“å“å“æŠŠå‰èœã€ä¸»èœã€ç”œç‚¹å…¨ç«¯ä¸Šæ¥ã€‚
    *   æ•ˆç‡ Maxï¼ğŸ”¥

### 3. ä½¿ç”¨å§¿åŠ¿ (Capture & Replay) ğŸ§˜
PyTorch æä¾›äº† `torch.cuda.CUDAGraph()` æ¥å®ç°è¿™ä¸ªé»‘é­”æ³•ã€‚

**å…³é”®æ­¥éª¤ï¼š**
1.  **Warmup (çƒ­èº«):** å…ˆè·‘å‡ æ¬¡ï¼Œè®© PyTorch è¯¥åˆ†é…çš„æ˜¾å­˜éƒ½åˆ†é…å¥½ (Cache èµ·æ¥)ã€‚
2.  **Capture (å½•åˆ¶):** å¼€å¯å½•åƒæ¨¡å¼ï¼Œè·‘ä¸€éæ¨¡å‹ã€‚
3.  **Replay (å›æ”¾):** ä»¥åå°±åªæ’­æ”¾å½•åƒã€‚

âš ï¸ **æ³¨æ„:** CUDA Graph å¯¹è¾“å…¥æ•°æ®çš„ **Shape** å’Œ **Memory Address (æŒ‡é’ˆ)** æœ‰ä¸¥æ ¼è¦æ±‚ï¼é€šå¸¸è¦æ±‚æ˜¯ Static Inputã€‚

## âš”ï¸ è¯¾åå®æˆ˜ (Action Items)

### 1. æ‰‹åŠ¨æŒ¡ CUDA Graph ğŸï¸
è¯•ç€è¿è¡Œè¿™æ®µä»£ç ï¼Œä½“éªŒä¸€ä¸‹ "å½•åˆ¶" çš„æ„Ÿè§‰ï¼š

```python
import torch
import time

# 1. å‡†å¤‡æ•°æ® (Static Input)
N, D_in, D_out = 64, 1024, 1024
# å¿…é¡»æ˜¯ CUDA tensor
x = torch.randn(N, D_in, device='cuda')
y = torch.randn(N, D_out, device='cuda')
model = torch.nn.Linear(D_in, D_out).cuda()
optimizer = torch.optim.SGD(model.parameters(), lr=1e-3)

# 2. Warmup (çƒ­èº«) - æå…¶é‡è¦ï¼è®© Pytorch åˆ†é…æ˜¾å­˜
# åŒæ—¶ä¹Ÿä½œä¸ºä¸€ä¸ª side_streamï¼Œé¿å…å¹²æ‰°é»˜è®¤æµ
s = torch.cuda.Stream()
s.wait_stream(torch.cuda.current_stream())
with torch.cuda.stream(s):
    for _ in range(3):
        optimizer.zero_grad(set_to_none=True)
        y_pred = model(x)
        loss = (y_pred - y).pow(2).sum()
        loss.backward()
        optimizer.step()
torch.cuda.current_stream().wait_stream(s)

# 3. Capture (å½•åˆ¶) ğŸ¥
g = torch.cuda.CUDAGraph()
# ç¡®ä¿æ‰€æœ‰æ˜¾å­˜åˆ†é…éƒ½åœ¨ Graph ä¹‹å¤–å®Œæˆ
optimizer.zero_grad(set_to_none=True)

with torch.cuda.graph(g):
    # ä¸‹é¢è¿™äº›æ“ä½œä¼šè¢«å½•è¿› g
    y_pred = model(x)
    loss = (y_pred - y).pow(2).sum()
    loss.backward()
    optimizer.step()

# 4. Replay (å›æ”¾) ğŸ¬
# ä¹‹åçš„æ¯ä¸€æ¬¡è¿­ä»£ï¼Œåªéœ€è¦ launch è¿™ä¸ª graph
print("Graph captured! Replaying...")
start = time.time()
for _ in range(1000):
    g.replay()
torch.cuda.synchronize() # ç­‰å¾… GPU è·‘å®Œ
print(f"1000 iter finished in {time.time() - start:.4f}s")
```

### 2. æ€è€ƒé¢˜ ğŸ¤”
*   ä¸ºä»€ä¹ˆ `torch.compile()` (Inductor) ç»å¸¸ä¹Ÿä¼šç”¨åˆ° CUDA Graphï¼Ÿ
*   å¦‚æœæˆ‘çš„ Input Image å¤§å°å˜äº† (æ¯”å¦‚ä» H=224 å˜æˆ H=256)ï¼Œè¿™ä¸ª Graph è¿˜èƒ½ç”¨å—ï¼Ÿ(ç­”æ¡ˆæ˜¯ï¼šâŒ å¿…é¡»é‡æ–° Captureï¼Œæˆ–è€…ç”¨ Dynamic Shape è¿™ç§é«˜çº§ç‰¹æ€§)ã€‚

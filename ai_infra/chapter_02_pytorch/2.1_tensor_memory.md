# 2.1 Tensor 内存布局 (Memory Layout) 📐

## 🎯 核心目标
理解 Tensor 在显存里到底是怎么摆放的。 `view()`, `reshape()`, `permute()` 到底有什么区别？

## 💡 概念解析

### 1. Stride (步长) 👣
Tensor 是逻辑上的多维数组，但显存是物理上的一维线性空间。
*   **Stride:** 为了在这个一维空间里找到下一行/下一列的元素，需要跳过多少个元素。

#### 📸 CV 工程师类比
*   **HWC vs CHW:**
    *   `[H, W, C]`: 像素点 RGB 是挨着的 (Channel Last)。
    *   `[C, H, W]`: R 通道整张图在一起，然后是 G，然后是 B (Channel First/Planar)。
    *   虽然 `print(tensor.shape)` 一样，但内存里完全不同！
    *   **坑:** 用 `cv2.imread()` 读进来是 HWC，PyTorch 也就是 `Conv2d` 需要 CHW。这就需要 `permute()`。

### 2. View vs Contiguous (视图 vs 连续) 👓
*   **View:** 只是改变了 Stride，**没有**搬运数据。零开销 O(1)。
*   **Contiguous:** 强制把数据在显存里重新排列成连续的。
    *   **注意:** 这通常会触发 **内存复制 (Copy)**，申请一块新的显存，把数据按顺序搬过去。它**不是** In-place 操作！
    *   代价: O(N) 的显存带宽开销 + O(N) 的显存占用。

#### 📸 CV 工程师类比
*   **Cropping (裁剪):** `img[:, 10:20, 10:20]`。
    *   这只是个 View。原图数据还在显存里，只是你只能看到这一小块。
    *   如果你想把这个 crop 拿去做某些特定的 CUDA 操作 (要求连续内存)，就需要 `.contiguous()`。这就好比你需要把这一小块截图 **另存为** 一张新图片。

### 3. Permute, Transpose & View 的三角关系 📐
*   **`permute()` / `transpose()`:** 也是 **View** 操作！它**只修改 Stride**，不搬运数据。
    *   例如 `x.permute(1, 0)` 会把 stride 从 `(3, 1)` 改成 `(1, 3)`。
    *   此时 Tensor 在物理上变得 **不再连续 (Non-contiguous)**。

*   **`view()` 的限制:** `view()` 只能处理 **Contiguous** 的 Tensor。
    *   这就是为什么你经常看到报错: `RuntimeError: view size is not compatible with input tensor's size and stride...`。
    *   **应对:** 先 call `.contiguous()` (强制拷贝整理内存)，然后再 call `.view()`。

### 4. Operator Fusion (算子融合前置) 🧩
为什么内存布局重要？
因为 GPU 喜欢 **Coalesced Access (合并访问)**。如果你的数据是乱序的 (非连续)，GPU 读取效率会暴跌 📉。

## ⚔️ 课后实战 (Action Items)
1.  **代码实验:**
    ```python
    import torch
    a = torch.randn(100, 100)
    b = a.t()  # 转置
    print(b.is_contiguous())  # False! 只是改了 Stride
    c = b.contiguous()        # 真的搬数据了
    print(c.is_contiguous())  # True
    ```
2.  **思考:** 为什么 NLP 里常说 `[Batch, Seq, Head, Dim]` 这种 Layout 比较好？(提示：为了让 Attention 计算时的访存更加连续)。

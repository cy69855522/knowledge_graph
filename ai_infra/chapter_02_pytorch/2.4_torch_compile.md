# 2.4 PyTorch 2.0 & torch.compile (Inductor) 🛠️

## 🎯 核心目标
为什么所有人都在安利 PyTorch 2.0？因为它能把 Python **编译** 成 Triton/C++，然后跑得飞快 ⚡。

## 💡 概念解析

### 1. Eager Mode vs Graph Mode (动态图 vs 静态图) 🎭
*   **Eager Mode (PyTorch 1.x):** 类似 Python 解释器。读一句，跑一句。灵活，方便 Debug，但慢 (Python Overhead, No Global Optimization)。
*   **Graph Mode (TensorFlow 1.x / ONNX):** 类似 C++ 编译。先把整个网络结构定义好，编译器进行全图优化 (Graph Optimization)，然后再跑。快，但反人类。

### 2. torch.compile() (The Best of Both Worlds) 🌟
PyTorch 2.0 的魔法在于 `torch.compile(model)`。它在 Eager Mode 运行的时候，偷偷抓取计算图 (Dynamo)，然后编译它。

#### 📸 CV 工程师类比
*   **Eager:** 就像你用纯 Python 写图像处理循环 `for x in range(W): for y in range(H): ...`。慢得令人发指。
*   **Graph (Inductor):** 就像你用 Cython 或者 Numba 把这段循环编译成了 C++。
*   **Inductor:** 这是一个**后端**，它可以把 PyTorch 算子融合之后，**生成 Triton 代码** (专门为 GPU 优化的 Python-like 语言)。

### 3. Triton (海卫一) 🔱
OpenAI 搞出来的，专为 GPU 编程设计的语言。
以前你需要写复杂的 CUDA C++ 才能利用好 GPU。现在用 Triton (Python 语法) 就能写出性能接近手撸 CUDA 的 Kernel。
**Inductor 自动帮你写 Triton 代码。**

## ⚔️ 课后实战 (Action Items)
1.  **一行代码提速:**
    找一个现有的 ResNet 或者 BERT 模型，加上一行：
    ```python
    model = torch.compile(model)
    ```
    跑跑看，第一轮会慢 (Compilation Overhead)，第二轮开始起飞 🛫。
2.  **Dig Deeper:** 设置环境变量 `TORCH_LOGS="+inductor"`，看看 PyTorch 到底生成了什么鬼东西。你会看到它不仅把算子融合了，还把内存读写优化到了极致。
